{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACES P3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## default config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aces.environement.p3.aces_p3 import ACES_p3\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class AcesArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.DataTrainingArguments\n",
    "    \"\"\"\n",
    "    environement_name : str = field( default = \"p3\", metadata={\"help\": \"environment name\"})\n",
    "    path_archive : str = field(\n",
    "        default = \"\", \n",
    "        metadata={\"help\": \"path to the archive if empty load the default archive\"}\n",
    "    )\n",
    "    path_save: str = field( \n",
    "        default = \"\",\n",
    "        metadata={\"help\": \"path to save the archive\"}\n",
    "    )\n",
    "    name_experience: str = field( \n",
    "        default = \"aces_P3_expe\",\n",
    "        metadata={\"help\": \"name of the experience (use for saving)\"}\n",
    "    )\n",
    "    n_generation: int = field( \n",
    "        default = 100,\n",
    "        metadata={\"help\": \"number of generation to run\"}\n",
    "    )\n",
    "    num_solutions: int = field(\n",
    "        default = 50, metadata={\"help\": \"number of solutions to generate to compute the difficulty score\"}\n",
    "    )\n",
    "    batch_size: int = field( \n",
    "        default = 32, \n",
    "        metadata={\"help\": \"number of query to send to the LLM to create new puzzles (multiple this number by 5 to get the number of generated puzzles as 5 puzzles are generated per query)\"})\n",
    "    n_fewshot_examples: int = field( default = 3, metadata={\"help\": \"number of example in context\" })\n",
    "    max_descriptor_targeted: int = field(\n",
    "        default = 5,\n",
    "        metadata={\"help\": \"number of max descriptor to target (at most `max_descriptor_targeted` semantic descriptor sample as goal)\"})\n",
    "    mode_sampling_goal: str = field(\n",
    "        default = \"uniform\",\n",
    "        metadata={\"help\": \"['uniform','smart','none'], uniform sample goal uniformely, smart: sample unexplored goal close that are within 1 of distance of already explored goal in the semantic space\"})\n",
    "    seed: int = field(default=0)\n",
    "    sampling_strategy_examples_from_niche: str = field(\n",
    "        default='soft_normalised',\n",
    "        metadata={\"help\": \"sampling strategy to sample examples from a niche, choice: 'uniform','prob_best_5','soft_normalised'; need to explain difference\"}\n",
    "    )\n",
    "    temperature_sampling_strategy_examples_from_niche: float = field(\n",
    "        default= 0.2, \n",
    "        metadata={\"help\": \"temperature softmax to sample example given their fitness given a niche\"}\n",
    "    )\n",
    "    puzzle_generation_strategy: str = field(\n",
    "    default= \"aces_elm\", \n",
    "    metadata={\"help\":\"startegy to generate new puzzle, choice: ['aces','aces_elm'] todo 'wizard_coder'\"})\n",
    "    difficulty_min_target: int = field(default = 90, metadata={\"help\":\"difficulty min to target /100\"})\n",
    "    difficulty_max_target: int = field(default = 100, metadata={\"help\":\"difficulty min to target /100\"})\n",
    "    save_every_n_generations: int = field(default = 1, metadata={\"help\":\"save archive every n generations\"})\n",
    "    path_checkpoint_archive: str = field(\n",
    "        default=\"\",\n",
    "        metadata={\"help\":\"if != '' resume experiment from the given a archive checkpoint \"})\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class LLMArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.DataTrainingArguments\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        default=\"/home/flowers/work/hf/Qwen2.5-0.5B-Instruct\",#\"/home/flowers/work/hf/Qwen2.5-Coder-3B-Instruct\",\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    online: Optional[bool] = field(\n",
    "        default = True,\n",
    "        metadata={\n",
    "            \"help\": \"use vllm server or OpenAI API if True else use offline vllm\"\n",
    "        },\n",
    "    )\n",
    "    base_url: Optional[str] = field(\n",
    "        default=\"http://localhost:8000\",\n",
    "        metadata={\n",
    "            \"help\": \"base url for vllm server\"\n",
    "        },\n",
    "    )\n",
    "    api_key: Optional[str] = field(\n",
    "        default=\"\",\n",
    "        metadata={\n",
    "            \"help\": \"api key \"\n",
    "        },\n",
    "    )\n",
    "    gpu: Optional[int] = field(\n",
    "        default = 1,\n",
    "        metadata={\n",
    "            \"help\": \"number of gpus to use (vllm)\"\n",
    "        },\n",
    "    )\n",
    "    temperature: Optional[float] = field(\n",
    "        default = 1.0,\n",
    "        metadata={\n",
    "            \"help\": \"temperature\"\n",
    "        },\n",
    "    )\n",
    "    temperature_labeller: Optional[float] = field(\n",
    "        default = 0.,\n",
    "        metadata={\n",
    "            \"help\": \"temperature labeller (semantic descriptor)\"\n",
    "        },\n",
    "    )\n",
    "    min_p: Optional[float] = field(\n",
    "        default = 0.05,\n",
    "        metadata={\n",
    "            \"help\": \"min_p\"\n",
    "        },\n",
    "    )\n",
    "    max_tokens: Optional[int] = field(\n",
    "        default = 4000,\n",
    "        metadata={\n",
    "            \"help\": \"max tokens\"\n",
    "        },\n",
    "    )\n",
    "    max_model_length: Optional[int] = field(\n",
    "        default = 25000,\n",
    "        metadata={\n",
    "            \"help\": \"max context size\"\n",
    "        },\n",
    "    )\n",
    "    swap_space: Optional[float] = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            \"help\": \"swap space (RAM memory for cache)\"\n",
    "        }\n",
    "    )\n",
    "    azure: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"use azure if True else use local vllm\"\n",
    "        },\n",
    "    )\n",
    "    local_server: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"use openai_api if True\"\n",
    "        },\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using closed source model API (e.g OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from key import api_key # import your api key from a file called key.py in the same directory as this script\n",
    "model_name_or_path = \"gpt-4.1\" #choose your model https://platform.openai.com/docs/models/gp\n",
    "path_save = \"/home/flowers/work/aces/save_data/\" # path to save the archive\n",
    "path_checkpoint_archive = \"/home/flowers/work/aces/save_data/aces_P3_gpt41_0/generation_20.pkl\"#\"/home/flowers/work/aces/save_data/aces_P3_expe_0/generation_7.pkl\" # \"if != '' resume experiment from the given a archive checkpoint \"\n",
    "name_experience = \"aces_P3_gpt41\" # name of the experience\n",
    "api_key = api_key # your api key\n",
    "base_url = \"https://petunia-julien.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview\" # let that empty (\"\") if you don't self-host an OpenAI compatible server\n",
    "aces_args = AcesArguments(path_save=path_save, name_experience=name_experience,path_checkpoint_archive=path_checkpoint_archive) \n",
    "llm_args = LLMArguments(model_name_or_path=model_name_or_path,api_key=api_key,base_url=base_url)\n",
    "print(\"args:\")\n",
    "print(aces_args)\n",
    "print(llm_args)\n",
    "aces= ACES_p3(aces_args, llm_args)\n",
    "aces.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path_archive = \"/home/flowers/work/aces/save_data/aces_P3_gpt41_0/generation_21.pkl\"\n",
    "with open(path_archive, \"rb\") as f:\n",
    "    archive  = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(650,704):\n",
    "#     if archive[i].fitness > -0.5:\n",
    "        \n",
    "#         print(archive[i].description)\n",
    "#         print(archive[i].fitness)\n",
    "#         print(archive[i].program_str)\n",
    "#         print(\"--\"*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aces",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
